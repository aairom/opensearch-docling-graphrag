# OpenSearch-Docling-GraphRAG

A comprehensive document processing and Retrieval-Augmented Generation (RAG) system that combines advanced document parsing, vector search, and knowledge graph capabilities.

## ğŸŒŸ Features

- **ğŸ“„ Advanced Document Processing**: Process various document formats (PDF, DOCX, PPTX, HTML, Markdown) using Docling
- **ğŸ” Vector Search**: Semantic search powered by OpenSearch with embedding-based retrieval
- **ğŸ•¸ï¸ Knowledge Graph**: Build and explore knowledge graphs using Neo4j
- **ğŸ’¬ RAG Question Answering**: Context-aware responses using local LLMs via Ollama
- **ğŸ“Š Batch Processing**: Process multiple documents efficiently
- **ğŸ¨ Modern UI**: User-friendly Streamlit interface
- **ğŸ³ Containerized**: Docker and Kubernetes ready
- **âš¡ Background Job Queue**: Async document processing with progress tracking
- **â±ï¸ Timestamped Outputs**: All results saved with timestamps

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    subgraph "User Interface"
        UI[Streamlit UI]
    end
    
    subgraph "Document Processing"
        Docling[Docling Processor]
        InputDocs[Input Documents]
    end
    
    subgraph "Storage & Retrieval"
        OS[OpenSearch<br/>Vector Store]
        Neo4j[Neo4j<br/>Knowledge Graph]
    end
    
    subgraph "AI/ML"
        Ollama[Ollama<br/>LLM & Embeddings]
    end
    
    subgraph "Results"
        OutputFiles[Timestamped Results]
    end
    
    InputDocs --> Docling
    Docling --> OS
    Docling --> Neo4j
    Docling --> OutputFiles
    
    UI --> Docling
    UI --> OS
    UI --> Neo4j
    UI --> Ollama
    
    OS --> Ollama
    Ollama --> UI
    
    style UI fill:#e1f5ff
    style Docling fill:#fff3e0
    style OS fill:#f3e5f5
    style Neo4j fill:#e8f5e9
    style Ollama fill:#fce4ec
    style OutputFiles fill:#fff9c4
```

## ğŸ“‹ Prerequisites

- **Python 3.11, 3.12, or 3.13** (Python 3.14+ not yet supported by dependencies)
- Docker & Docker Compose
- Ollama with models installed:
  - `ibm/granite4:latest` (or your preferred LLM)
  - `granite-embedding:278m` (or alternative embedding model)
- 8GB+ RAM
- 10GB+ disk space

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd opensearch-docling-graphrag
```

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env with your configuration
```

### 3. Start the Application

```bash
./start.sh
```

The script will:
- Create necessary directories
- Set up Python virtual environment
- Install dependencies
- Start Docker services (OpenSearch, Neo4j)
- Launch the Streamlit application

Access the application at: **http://localhost:8501**

### 4. Stop the Application

```bash
./stop.sh
```

## ğŸ“– Usage

### Upload Single Document

1. Navigate to the "Upload" tab
2. Select a document file
3. Click "Process Document"
4. View results and explore the knowledge graph

### Batch Processing

1. Place documents in the `./input` directory
2. Navigate to "Batch Process" tab
3. Click "Process All Files"
4. Results saved to `./output` with timestamps

### Search & Query

1. Navigate to "Search" tab
2. Enter your question
3. View AI-generated answer with sources
4. Explore related documents

### Knowledge Graph

1. Navigate to "Graph Explorer" tab
2. View graph statistics
3. Search for entity connections
4. Explore document relationships

## ğŸ—ï¸ Project Structure

```
opensearch-docling-graphrag/
â”œâ”€â”€ app.py                      # Main Streamlit application
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example               # Environment configuration template
â”œâ”€â”€ start.sh                   # Application startup script
â”œâ”€â”€ stop.sh                    # Application shutdown script
â”œâ”€â”€ github-deploy.sh           # GitHub deployment automation
â”œâ”€â”€ Dockerfile                 # Application container
â”œâ”€â”€ docker-compose.yml         # Multi-service orchestration
â”‚
â”œâ”€â”€ config/                    # Configuration modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py           # Application settings
â”‚
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ processors/           # Document processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ docling_processor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/                  # RAG components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ opensearch_client.py
â”‚   â”‚   â””â”€â”€ ollama_client.py
â”‚   â”‚
â”‚   â”œâ”€â”€ graphrag/             # Knowledge graph
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ neo4j_client.py
â”‚   â”‚   â””â”€â”€ graph_builder.py
â”‚   â”‚
â”‚   â””â”€â”€ ui/                   # UI components
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ k8s/                      # Kubernetes manifests
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”œâ”€â”€ secrets.yaml
â”‚   â”œâ”€â”€ opensearch-deployment.yaml
â”‚   â”œâ”€â”€ neo4j-deployment.yaml
â”‚   â””â”€â”€ app-deployment.yaml
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”œâ”€â”€ input/                    # Input documents directory
â”œâ”€â”€ output/                   # Processed results (timestamped)
â””â”€â”€ logs/                     # Application logs
```

## ğŸ³ Docker Deployment

### Using Docker Compose

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Build Custom Image

```bash
docker build -t docling-rag:latest .
docker run -p 8501:8501 docling-rag:latest
```

## â˜¸ï¸ Kubernetes Deployment

See [k8s/README.md](k8s/README.md) for detailed Kubernetes deployment instructions.

Quick deploy:

```bash
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secrets.yaml
kubectl apply -f k8s/opensearch-deployment.yaml
kubectl apply -f k8s/neo4j-deployment.yaml
kubectl apply -f k8s/app-deployment.yaml
```

## ğŸ”§ Configuration

### Environment Variables

Key configuration options in `.env`:

```bash
# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=ibm/granite4:latest
OLLAMA_EMBEDDING_MODEL=granite-embedding:278m

# OpenSearch
OPENSEARCH_HOST=localhost
OPENSEARCH_PORT=9200
OPENSEARCH_USER=admin
OPENSEARCH_PASSWORD=admin

# Neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# Processing
CHUNK_SIZE=512
CHUNK_OVERLAP=50
```

## ğŸ“Š System Requirements

### Minimum

- CPU: 4 cores
- RAM: 8GB
- Storage: 10GB

### Recommended

- CPU: 8+ cores
- RAM: 16GB+
- Storage: 50GB+
- GPU: Optional (for faster embeddings)

## ğŸ” Supported Document Formats

- PDF (`.pdf`)
- Microsoft Word (`.docx`)
- Microsoft PowerPoint (`.pptx`)
- HTML (`.html`)
- Markdown (`.md`)
- Plain Text (`.txt`)
- AsciiDoc (`.adoc`)
- Images (with OCR)

## ğŸ› ï¸ Development

### Setup Development Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Run Tests

```bash
pytest
```

### Code Style

```bash
# Format code
black .

# Lint
flake8 .
```

## ğŸ“ API Documentation

### Document Processing

```python
from src.processors import DoclingProcessor

processor = DoclingProcessor()
result = processor.process_document("path/to/document.pdf")
```

### Vector Search

```python
from src.rag import OpenSearchClient, OllamaClient

os_client = OpenSearchClient()
ollama_client = OllamaClient()

# Generate embedding
embedding = ollama_client.generate_embedding("query text")

# Search
results = os_client.search(embedding, k=5)
```

### Knowledge Graph

```python
from src.graphrag import Neo4jClient, GraphBuilder

neo4j_client = Neo4jClient()
graph_builder = GraphBuilder(neo4j_client)

# Build graph
graph_builder.build_document_graph(
    document_id="doc1",
    file_name="document.pdf",
    file_path="/path/to/document.pdf",
    chunks=chunks
)
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- [Docling](https://github.com/docling-project/docling) - Document processing
- [OpenSearch](https://github.com/opensearch-project/OpenSearch) - Vector search
- [Neo4j](https://github.com/neo4j/neo4j) - Knowledge graph
- [Ollama](https://ollama.ai/) - Local LLM inference
- [Streamlit](https://streamlit.io/) - Web UI framework

## ğŸ“ Support

For issues, questions, or contributions:
- Open an issue on GitHub
- Check the [documentation](docs/)
- Review [k8s/README.md](k8s/README.md) for deployment help

## ğŸ—ºï¸ Roadmap

- [ ] Multi-language support
- [ ] Advanced entity extraction with NER models
- [ ] Real-time document monitoring
- [ ] API endpoints (REST/GraphQL)
- [ ] Enhanced visualization
- [ ] Performance optimizations
- [ ] Cloud deployment templates (AWS, GCP, Azure)

---

**Built with â¤ï¸ for the AI community**